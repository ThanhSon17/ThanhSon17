<LinearLayout
    xmlns:android="http://schemas.android.com/apk/res/android"
    android:layout_width="match_parent"
    android:layout_height="match_parent"
    android:orientation="vertical"
    android:gravity="center"
    android:padding="16dp">

    <Button
        android:id="@+id/pickImageButton"
        android:layout_width="wrap_content"
        android:layout_height="wrap_content"
        android:text="Chọn ảnh" />

    <ImageView
        android:id="@+id/imageView"
        android:layout_width="200dp"
        android:layout_height="200dp"
        android:layout_marginTop="16dp"
        android:scaleType="centerCrop"
        android:contentDescription="Ảnh được chọn" />
</LinearLayout>

private lateinit var imageView: ImageView

    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        setContentView(R.layout.activity_main)

        val pickImageButton: Button = findViewById(R.id.pickImageButton)
        imageView = findViewById(R.id.imageView)

        // Activity result launcher to handle image picking
        val pickImageLauncher = registerForActivityResult(ActivityResultContracts.StartActivityForResult()) { result ->
            if (result.resultCode == Activity.RESULT_OK) {
                val selectedImageUri: Uri? = result.data?.data
                selectedImageUri?.let {
                    imageView.setImageURI(it)
                }
            }
        }

        // Set button click listener to open gallery
        pickImageButton.setOnClickListener {
            val intent = Intent(Intent.ACTION_PICK).apply {
                type = "image/*"
            }
            pickImageLauncher.launch(intent)
        }
    }
}


val pickImageLauncher = registerForActivityResult(ActivityResultContracts.StartActivityForResult()) { result ->
            if (result.resultCode == Activity.RESULT_OK) {
                val selectedImageUri: Uri? = result.data?.data
                selectedImageUri?.let {
                    imageView.setImageURI(it)

                    // Get the real path and title of the image
                    val realPath = getRealPathFromURI(it)
                    val title = getFileNameFromURI(it)

                    // Add image details to the path list
                    val pathItem = PathItem(
                        id = it.lastPathSegment ?: "unknown_id",
                        mUrl = realPath ?: "unknown_path",
                        mTitle = title ?: "unknown_title",
                        mimeType = contentResolver.getType(it) ?: "unknown"
                    )
                    pathList.add(pathItem)
                }
            }
        }

        // Set button click listener to open gallery
        pickImageButton.setOnClickListener {
            val intent = Intent(Intent.ACTION_PICK).apply {
                type = "image/*"
            }
            pickImageLauncher.launch(intent)
        }
    }

    // Helper function to get real file path from URI
    private fun getRealPathFromURI(uri: Uri): String? {
        var path: String? = null
        val projection = arrayOf(MediaStore.Images.Media.DATA)
        val cursor: Cursor? = contentResolver.query(uri, projection, null, null, null)
        cursor?.use {
            if (it.moveToFirst()) {
                val columnIndex = it.getColumnIndexOrThrow(MediaStore.Images.Media.DATA)
                path = it.getString(columnIndex)
            }
        }
        return path
    }

    // Helper function to get file name from URI
    private fun getFileNameFromURI(uri: Uri): String? {
        var name: String? = null
        val projection = arrayOf(MediaStore.Images.Media.DISPLAY_NAME)
        val cursor: Cursor? = contentResolver.query(uri, projection, null, null, null)
        cursor?.use {
            if (it.moveToFirst()) {
                val columnIndex = it.getColumnIndexOrThrow(MediaStore.Images.Media.DISPLAY_NAME)
                name = it.getString(columnIndex)
            }
        }
        return name
    }



1. AI Trợ Lý Ảo với ChatGPT hoặc LLMs
Bạn có thể tích hợp một mô hình AI như GPT-4 hoặc Gemini để giúp trợ lý ảo có thể trả lời câu hỏi tự nhiên.
Dùng API của OpenAI hoặc một mô hình LLM chạy cục bộ để tránh độ trễ khi giao tiếp.
2. Tích Hợp với Nhà Thông Minh (Smart Home)
Sử dụng các nền tảng như Home Assistant, Google Home, hoặc Apple HomeKit để kết nối và điều khiển thiết bị trong nhà.
Dùng giao thức Matter hoặc MQTT để kết nối với nhiều thiết bị khác nhau.
3. Nhân Vật Ảo 3D Di Chuyển trong Không Gian
Sử dụng Unity hoặc Unreal Engine để tạo nhân vật 3D xuất hiện trong thế giới thực tế ảo.
Công nghệ AI NPC (Non-Player Character) của NVIDIA ACE hoặc Inworld AI có thể giúp nhân vật này có phản ứng tự nhiên hơn.
Dùng Inverse Kinematics (IK) và Animation AI để nhân vật có thể di chuyển linh hoạt.
4. Tích Hợp trên Meta Quest
Meta Quest 3 hỗ trợ WebXR, OpenXR, nên có thể phát triển ứng dụng thực tế ảo bằng Unity hoặc WebXR.
Nếu muốn chạy offline, bạn cần biên dịch mô hình AI xuống thiết bị (ví dụ: chạy mô hình Llama 3 trên nền tảng Android của Quest).
5. Điều Khiển bằng Giọng Nói và Cử Chỉ
Sử dụng Meta Voice SDK hoặc OpenAI Whisper để nhận diện giọng nói.
Kết hợp với công nghệ Hand Tracking và Eye Tracking trên Quest để giúp người dùng tương tác với trợ lý dễ hơn.

Kính Meta Quest 3 có thể trở thành một công cụ hỗ trợ mạnh mẽ cho người khiếm thị bằng cách sử dụng AI, Computer Vision (thị giác máy tính) và công nghệ âm thanh không gian để:

Nhận diện môi trường xung quanh: Xác định vật thể, chướng ngại vật, con người và biển báo trong thời gian thực.
Đọc văn bản & biển hiệu: Sử dụng OCR (Optical Character Recognition) để đọc sách, menu, bảng chỉ dẫn, tài liệu, v.v.
Mô tả cảnh vật & hướng dẫn đường đi: Cung cấp mô tả bằng giọng nói AI, hướng dẫn người dùng di chuyển an toàn.
Nhận diện người & đồ vật quen thuộc: Nhắc nhở khi gặp người quen hoặc đồ vật thường dùng (chìa khóa, điện thoại...).
Hỗ trợ AI tương tác bằng giọng nói: Người dùng có thể đặt câu hỏi, tìm địa điểm gần nhất hoặc yêu cầu trợ giúp bằng lệnh thoại.
Công nghệ cần sử dụng
1. Computer Vision (Thị giác máy tính)
✔ YOLO (You Only Look Once), OpenCV, TensorFlow, Mediapipe – Nhận diện vật thể, biển báo giao thông, khuôn mặt người.
✔ Depth Sensors (Cảm biến chiều sâu của Meta Quest 3) – Đánh giá khoảng cách để tránh va chạm.

2. AI & Xử lý ngôn ngữ tự nhiên (NLP)
✔ Whisper (OpenAI), Google Speech-to-Text – Nhận diện giọng nói để người khiếm thị ra lệnh bằng giọng.
✔ ChatGPT, Gemini AI, GPT-4 – Xử lý ngôn ngữ, đưa ra phản hồi thông minh bằng giọng nói.

3. OCR (Nhận diện chữ viết)
✔ Tesseract OCR, Google Cloud Vision API – Đọc văn bản trên biển hiệu, tài liệu, sách, hóa đơn.

4. Hệ thống âm thanh định hướng & cảnh báo thông minh
✔ Spatial Audio (Âm thanh không gian của Meta Quest 3) – Tạo âm thanh hướng dẫn phát ra từ phía vật thể để giúp định hướng tốt hơn.
✔ Bone Conduction Headphones (Tai nghe truyền âm qua xương) – Cho phép nghe thông báo mà không cần che tai, giúp giữ an toàn khi đi đường.

5. Điều hướng & GPS
✔ Google Maps API, OpenStreetMap, ARKit/ARCore – Hướng dẫn đường đi theo thời gian thực.
✔ SLAM (Simultaneous Localization and Mapping) – Công nghệ điều hướng trong nhà, giúp người dùng di chuyển trong siêu thị, bến xe, trung tâm thương mại.

Cách hoạt động của hệ thống
1️⃣ Nhận diện môi trường: Camera & AI phân tích cảnh vật.
2️⃣ Cảnh báo chướng ngại vật: Hệ thống âm thanh thông báo "Phía trước có xe đạp, cách 2 mét".
3️⃣ Đọc biển báo & hướng dẫn đường đi: Nếu thấy biển hiệu "Lối ra", AI đọc lên: "Lối ra ở phía trước, đi thẳng 5 mét rồi rẽ trái".
4️⃣ Trả lời câu hỏi của người dùng: Người dùng hỏi: "Nhà vệ sinh ở đâu?", AI tìm trên bản đồ và hướng dẫn.
5️⃣ Đọc sách, thực đơn, tài liệu: Người dùng hướng kính vào tờ giấy và ra lệnh "Đọc nội dung này", hệ thống sẽ quét & đọc to.

Ứng dụng thực tế
✅ Hỗ trợ người khiếm thị đi lại an toàn trong thành phố
✅ Giúp đọc văn bản, sách, tài liệu mà không cần trợ giúp từ người khác
✅ Xác định được người thân, bạn bè hoặc đồ vật cá nhân
✅ Hướng dẫn di chuyển trong nhà ga, sân bay, trung tâm thương mại
